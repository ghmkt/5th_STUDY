{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "토큰화는 구두점(./,/?/;/! 등)의 문자를 제외시키는 작업을 말한다. \n",
    "그런데 구두점이나 특수문자를 전부 제거하는 정제 작업을 수행하는 것만으로는 부족하다. \n",
    "구두점이나 특수문자를 제거하면 토큰이 의미를 잃어버리는 경우도 발생하기 떄문.\n",
    "심지어 한국어는 영어랑 달리 띄어쓰기로 구분한다고 단어 토큰이 구분되지 않는다.(왤까? 그 이유는 뒤애)\n",
    "토큰화 중 사용하는 라이브러리 마다 다르게 토큰화가 이루어질 수 있다.\n",
    "ex)어떤 라이브러리는 Do  n't로 다른 라이브러리는 don 't로 구분하는 식으로 다름.\n",
    "이제 토큰화를 할 때 주의해야할 점을 살펴보자. \n",
    "1.구두점이나 특수 문자를 단순제외해서는 안된다. 온점(.)같은 경우는 문장의 경계를 아는데 쓰일 수 있고 단어 자체에서 구두점을 갖고 있는 Ph.D같은 경우들이 있고, $45같은 가격 표시 혹은 01/02/06 같은 날짜의미가 있는데 이때 01/02/06을 하나로 취급해야지 따로 분류해서는 원래 의도와 멀어지게 된다. 마찬가지로 숫자 사이에 12,345,566 같이 컴마가 들어가 있는 경우도 존재하기에 ,에도 유의. \n",
    "2.줄임말과 단어 내에 띄어쓰기가 있는 경우.\n",
    "영어에서는 whar're처럼 줄임멀인 접어(clitic)이 존재하기 때문에 토큰화는 이런 압축된 단어를 펼처서 구분하는 역할을 해주어야한다. 또 New York 처럼 중간에 띄어쓰기가 있지만 하나의 단어로 취급해야하는 경우가 있다. \n",
    "****\n",
    "한국어에서 토큰화가 어려운 이유.\n",
    "1) 한국어는 교착어이다. \n",
    "한국어는 조사라는 것이 존재하기 때문에 '그에게','그를','그와' 등이 '그'에 붙게 된다. \n",
    "이때 자연어처리를 할때 이런 것들을 모두 다른 단어로 인식하면 힘들고 번거로워진다. \n",
    "따라서 조사를 분리할 필요가 있다. 즉 한국어 토큰화는 형태소라는 개념을 이해해야한다.\n",
    "자립 형태소: 접사 어미 조사와 상관없이 자립하여 사용할 수 있는 형태소를 말하고 그 자체로 단어가 된다. 체언(명사,대명사,수사)와 수식언(관형사,부사), 감탄사 등이 있다.\n",
    "의존 형태소: 다른 형태소와 결합하여 사용되는 형태소\n",
    "\n",
    "2)한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.\n",
    "한국어는 띄어쓰기가 지켜지지 않는 경향이 있고 이는 영어와는 달리 띄어쓰기가 안되어있어도 알아볼 수 있기 떄문.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##koNLPy를 이용한 한국어 토큰화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt  \n",
    "okt=Okt()  \n",
    "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정제와 정규화 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.\n",
    "정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다\n",
    "\n",
    "이런 정제와 정규화는 토큰화 작업 전 후 같이 해주어야한다. \n",
    "\n",
    "대 소문자 통합은 Automobile이란 단어와 automobile이란 단어를 같은 단어로 만들어줌으로써 불필요한 낭비를 피해줄 수 있다. \n",
    "하지만 무조건 통합해서는 안되는게 US와 us처럼 다른 뜻이 되는 경우도 있고, 회사이름이나 사람이름은 대문자로 유지되는게 좋기 때문이다.\n",
    "보통 처음 단어 대문자는 소문자로 유지하고 문장 속에 나오는 대문자들은 그대로 유지하는 방법을 많이 쓴다. \n",
    "# 불필요한 단어의 제거를 해보자\n",
    "1)등장 빈도가 적은 단어\n",
    "굉장히 적은 빈도로 등장하는 단어는 binary 분류 등에 큰 영향을 주지 않을 것. \n",
    "그런데 역시 일률적인 것이 아니라 적은 빈도 단어를 차출하는 것이 필요할때도 있을 것.\n",
    "2)길이가 짧은 단어\n",
    "보통은 길이가 짧은 단어의 경우 크게 의미가 없는 경우가 많은데 영어가 아닌 한국어의 경우는 이야기가 많이 다르다. 특히 한국어는 영어의 평균 길이 6-7에 비해서 2-3정도로 훨씬 짧기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어간 추출과 표제어 추출"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "표제어 추출(Lemma):표제어는 기본사전형 단어 정도의 의미이다. \n",
    "예를들면 am, are, is등의 표제어가 be라고 할 수 있다. 이를 위해서는\n",
    "형태학적 파싱이 먼저 진행되어야하는데 형태소는 어간과 접사로 나눌 수 있는데 이 2개를 분리하는 작업을 말한다. cats를 cat -s로 구분하는 것. \n",
    "어간 추출(Stemming): 위에서 나온 어간을 추출하는 작업을 어간 추출이라고 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "문장에서 자주 등장하지만 큰 의미가 없는 단어들이 있다. \n",
    "예를 들면 i,my, me, over등은 실제 의미 분석을 하는데 거의 기여하는 바가 없다. 이러한 단어들을 불용어(stopword)라고 하며 nltk에서는 100개이 상의 영어 단어들을 불용어로 패키지 내에서 미리 정의하고 있다. \n",
    "그런데 이런 불용어를 처리할때는 늘 목적에 맞게 해야한다. 내가 챗봇을 만들고 싶은데 i,my,me등을 없앤다고 무조건 좋은 방향의 결과가 나오리라는 보장이 없기 때문이다. 따라서 자연어 처리를 할때는 첫 단어 시작 부터 방향성을 고민해가면서 전처리를 해야하는 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk  \n",
    "from nltk.corpus import stopwords  \n",
    "stopwords.words('english')[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
      "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n",
    "# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\n",
    "stop_words=stop_words.split(' ')\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = [] \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "# 위의 4줄 코드는 아래의 한 줄로 대체 가능\n",
    "# result=[word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print(word_tokens) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터의 분리"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "데이터의 분리는 우리가 주로 하는 suprvised learning 즉 지도 학습에서 중요하다.\n",
    "지도 학습은 주어진 데이터에 대한 답을 설정하고 여기서 weight를 조정하는 식으로 하기 때문이다. validation set도 포함하는 경우도 있지만 여기서는 원초적으로 훈련 데이터와 테스트 데이터만 나누도록 하자. \n",
    "<훈련 데이터>\n",
    "X_train : 문제지 데이터\n",
    "y_train : 문제지에 대한 정답 데이터.\n",
    "\n",
    "<테스트 데이터>\n",
    "X_test : 시험지 데이터.\n",
    "y_test : 시험지에 대한 정답 데이터."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'b', 'c')\n",
      "(1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "X,y = zip(['a', 1], ['b', 2], ['c', 3])\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>메일 본문</th>\n",
       "      <th>스팸 메일 유무</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>당신에게 드리는 마지막 혜택!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>내일 뵐 수 있을지 확인 부탁드...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>도연씨. 잘 지내시죠? 오랜만입...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(광고) AI로 주가를 예측할 수 있다!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    메일 본문  스팸 메일 유무\n",
       "0        당신에게 드리는 마지막 혜택!         1\n",
       "1    내일 뵐 수 있을지 확인 부탁드...         0\n",
       "2    도연씨. 잘 지내시죠? 오랜만입...         0\n",
       "3  (광고) AI로 주가를 예측할 수 있다!         1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "values = [['당신에게 드리는 마지막 혜택!', 1],\n",
    "['내일 뵐 수 있을지 확인 부탁드...', 0],\n",
    "['도연씨. 잘 지내시죠? 오랜만입...', 0],\n",
    "['(광고) AI로 주가를 예측할 수 있다!', 1]]\n",
    "columns = ['메일 본문', '스팸 메일 유무']\n",
    "\n",
    "df = pd.DataFrame(values, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "[[ 0  1  2]\n",
      " [ 4  5  6]\n",
      " [ 8  9 10]\n",
      " [12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ar = np.arange(0,16).reshape((4,4))\n",
    "print(ar)\n",
    "X=ar[:, :3]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]\n",
      " [8 9]]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = np.arange(10).reshape((5, 2)), range(5)\n",
    "# 실습을 위해 임의로 X와 y가 이미 분리 된 데이터를 생성\n",
    "print(X)\n",
    "print(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3]\n",
      " [4 5]\n",
      " [6 7]]\n",
      "[[8 9]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1234)\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정수 인코딩"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "컴퓨터는 텍스트를 처리할때 결국 ascii코드 상의 숫자로 변환하여 처리합니다. \n",
    "따라서 어차피 텍스트 자체는 컴퓨터의 bianry 연산에 직접적으로 바뀌지 못하고 연산을 할거면 더 의미 있는 숫자를 부여하여 연산을 한다면 더 잘 처리 할 수 있습니다. \n",
    "이를 위해 자연어 처리에서는 텍스트를 숫자로 바꾸는 여러가지 기법들이 있습니다. 그리고 그러한 기법들을 본격적으로 적용시키기 위한 첫 단계로 각 단어를 고유한 숫자에 맵핑(mapping)시키는 전처리 작업이 필요할 때가 있습니다.\n",
    "\n",
    "예를 들어 갖고 있는 텍스트에 단어가 5,000개가 있다면, 5,000개의 단어들 각각에 0번부터 4,999번까지 단어와 맵핑되는 고유한 숫자, 다른 말로는 인덱스를 부여합니다. (1번부터 시작해서 부여해도 상관은 없으며 코딩을 하는 동안 이를 인지하고 있으면 됩니다.) 가령, book은 150번, dog는 171번, love는 192번, books는 212번과 같이 숫자가 부여됩니다. 인덱스를 부여하는 방법은 여러 가지가 있을 수 있는데 랜덤으로 부여하기도 하지만, 보통은 전처리도 같이 겸하기 위해 단어에 대한 빈도수로 정렬한 뒤에 부여합니다.-->이는 빈도수가 낮은 것은 분석에서 제외하기 위함임. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Deep Learning for Natural Language process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "H(x)=Wx+b가 핵심. Weight를 최적으로 바꾸어 나가는게 목표. \n",
    "이때 input 사진이 한개라면 y가 (n,1)이 되고, n은 분류하고자 하는 label의 개수가 된다. \n",
    "Ex) cat,dog,ship이고, x는 (m,1)이 되는데 m은 input image의 픽셀개수가 된다. (혹은 한 input의 데이터를 일렬로 늘어뜨린 것.) 이때 따라서 weight은 (n,m)이 된다. 결과 label의 개수만큼 무조건 나올 수 밖에 없기 대문에 n을 맞추어야하고 m은 그만큼 곱해지는 것이 된다. \n",
    "이때 Wx=y=(n,1)이 된다는 게 중요하다. \n",
    "가장 처음 배워야하는 것은 sigmoid 함수. \n",
    "H(X)=1/(1+e−(Wx+b))=sigmoid(Wx+b)=σ(Wx+b)\n",
    "시그모이도 함수의 가장 큰 특징은 출력값을 S자 모양으로 0과 1사이의 값으로 조정하여 반환한다는 것이다. \n",
    "\n",
    "\n",
    "softmax classification은 binary classification 처럼 한 개의 상태이냐 아니냐를 판단하는 것이 아닌\n",
    "여러개의 상태 중 어떤 것에 속하는지 분류할때 사용됩니다. \n",
    "이떄 식은 h(x)=xW+b의 기본 가설함수를 따르지만 \n",
    "input x값은 n*input으로 주어지는 특성들의 개수 y와 h(x)는 n*구별해야할 라벨 개수\n",
    "따라서 W는 특성개수 *라벨 개수가 되고 이 행렬의 각 열은 가설행렬의 각 라벨을 결정하게 됩니다.\n",
    "\n",
    "이번에는 sigmoid가 아닌 softmax function을 사용하여 기본 logits 벡터들을 모두 0~1사이이고 합하여 1이 되는\n",
    "벡터로 바꾸도록 합니다. 이는 각 예측값을 합쳤을때 1이되는 확률 값으로 변환해주기 위함입니다.  \n",
    "softmax= e^yi/sigma(e^yj) \n",
    "결국 우리 가설함수 H(x)=softmax(h(x))가 됩니다.\n",
    "\n",
    "이제 cost 함수를 알아볼텐데 이번에 알아볼 cost 함수는 cross-entropy cost funciton으로\n",
    "D(H(x),y)=sigma(yi*(-log(H(x)))) 입니다.\n",
    "이때 i는 라벨의 개수만큼입니다.\n",
    "이해해보자면 라벨이 1으로 표시된 부분 즉, 실제 그 트레이닝 데이터의 정답인 부분의 예측함수가 1로부터 \n",
    "멀리 떨어진 만큼을 cost로 설정하는 것이고 이는 앞의 logistic cost와 결국 같은 메커니즘입니다.\n",
    "마지막으로 이를 전체 트레이닝 데이터의 개수만큼 모두 더하고 나누어 평균을 내면 \n",
    "우리가 원하는 최종 Loss=1/n*sigam(D(H(x),y))가 완성됩니다.\n",
    "\n",
    "[tensorflow code]\n",
    "#기존 logistic 부분과 다른 부분 위주로 설명하겠습니다. \n",
    "#주의할 부분은 이제 y의 데이터 형식이 n*label이므로 \n",
    "y=tf.placeholder(\"float\",[None,3])이런식으로 써주어야하고\n",
    "W=tf.Variable(tf.random_normal([input,label]),name='weight')로 적어야합니다.\n",
    "#이때 가설은 최종적으로 softmax한 것을 가설로 정하므로\n",
    "hypo=tf.nn.softmax(tf.matmul(x,W)+b)\n",
    "cost=tf.reduce_mean(-tf.reduce_sum(y*tf.log(hypo),axis=1))\n",
    "#이부분이 조금 이해하기 까다로울 수 있는데 axis=1은 행 안에서 합치라는 뜻입니다. \n",
    "#따라서 실제 y*H(x)의 모양이 [ [0 0.8 0], [0 0 0.7] ] 처럼 나오고 (elementwise곱이기 떄문)\n",
    "#이를 [0.8, 0.7] 이런식으로 만들어줍니다\n",
    "#즉 우리의 cross entropy cost function에서 라벨이 아닌 부분은 0이 곱해져 사라졋던 것을 행해주는 부분입니다.\n",
    "#그리고 이를 axis없이 트레이닝 데이터 개수로 평균내면 마지막 스칼라로 된 loss를 얻을 수 있습니다.\n",
    "\n",
    "#Fancy sofrmax\n",
    "#조금 더 세세한 부분을 다루어 보겠습니다.\n",
    "a=sess.run(hypo,feed_dict=feed)\n",
    "print(a,sess.run(tf.argmax(a,1)))\n",
    "#이는 행속에서 가장 높은 애의 index를 뱉어내줍니다. \n",
    "cost=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=y)\n",
    "#이 최신 버젼을 이용하면 tf.matmul(x,W)+b를 만들어 y와 넣으면 바로 스칼라 cost를 구해줍니다.\n",
    "#단 이때 y는 역시 one-hot encodig된 데이터여야겠죠! \n",
    "#만일 그러지 않은 경우 0~6까지 정수형을 되있을 경우에 one-hot화 하는 방법을 알아봅시다.\n",
    "y_before=tf.placeholder(tf.int32,[None,1]) # 0~6의 자료형이 주어졌을때\n",
    "y=tf.one_hot(y,label=7)\n",
    "y=tf.reshape(y,[-1,label=7])\n",
    "#그냥 온핫하면 [ [[100]] [[011] ] 처럼 차원을 하나 더 주기 때문에 reshape을 이용하여 n*7의 형태로 바꾸어줍시다.\n",
    "\n",
    "#나머지는 모두 같고 예측치와 정확도를 한번 구해봅시다.\n",
    "prediction=tf.argmax(hypo,1) #가장 높은 확률이 나온 애로 predict하자고 결정\n",
    "correct_prediction=tf.equal(prediction,tf.argmax(y,1))#실제 라벨값과 예측 값이 같으면 1 아니면 0의 행렬화\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))#다시 float으로 바꾸고 평균을 구한다.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

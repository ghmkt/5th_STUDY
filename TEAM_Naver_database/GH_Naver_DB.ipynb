{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------initializing------------------------\n",
      "                    created by - GH 5기 권기남 백승렬\n",
      "                                                     \n",
      "Designed to crawl naver hourly real-time data\n",
      "Saved destionation: GH bigquery NaverCrawler.Trending20\n",
      "Starting from 2019-01-01\n",
      "Each operation updates database to date of operation\n",
      "Time taken = 3 minutes per day\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------initializing------------------------\")\n",
    "print(\"                    created by - GH 5기 권기남 백승렬\")\n",
    "print(\"                                                     \")\n",
    "print(\"Designed to crawl Naver hourly real-time data\")\n",
    "print(\"Saved destionation: GH bigquery NaverCrawler.Trending20\")\n",
    "print(\"Starting from 2019-01-01\")\n",
    "print(\"Each operation updates database to date of operation\")\n",
    "print(\"Time taken = 3 minutes per day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import lxml\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import requests\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_gbq as gbq\n",
    "import numpy as np\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 'NaverCrawler-f54b101166fd.json' located in GH drive\n",
    "\n",
    "credential_json = 'your_bigquery_json_file.json' #downloaded and located in your path\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(credential_json)\n",
    "\n",
    "project_id = 'bigquery_project_id'\n",
    "\n",
    "client = bigquery.Client(credentials= credentials, project=project_id)\n",
    "\n",
    "table_name = 'your_table_name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## connect to chromedriver/ setup for crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "options.add_argument('headless')\n",
    "\n",
    "options.add_argument('window-size=1920x1080')\n",
    "\n",
    "path = 'your\\\\chromedriver\\\\path\\\\chromedriver.exe' # Set path to chromedriver. linux/ mac does not have .exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['Age', 'Datetime', 'Rank1', 'Rank2', 'Rank3', 'Rank4', 'Rank5', 'Rank6', 'Rank7', 'Rank8', 'Rank9', 'Rank10',\n",
    "            'Rank11', 'Rank12', 'Rank13', 'Rank14', 'Rank15', 'Rank16', 'Rank17', 'Rank18', 'Rank19', 'Rank20']\n",
    "times = ['00','01','02', '03','04', '05', '06', '07', '08', '09','10','11','12','13','14','15','16','17','18','19','20','21','22','23']\n",
    "table_list = ['all', '10s', '20s', '30s', '40s', '50s']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  function to get specific day's all age range's trending keywords (top 20) for each hour and append to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_update(input_date):\n",
    "    daily = []\n",
    "    \n",
    "    ### repeat for each hour (time)\n",
    "\n",
    "    for time in times:\n",
    "        \n",
    "        ### connect to chromedriver\n",
    "        \n",
    "        driver = webdriver.Chrome(path, options=options)\n",
    "\n",
    "        ### connect to trending keywords in specific input date and time, and parse\n",
    "        \n",
    "        driver.get(\"https://datalab.naver.com/keyword/realtimeList.naver?datetime={}T{}:00:00\".format(input_date, time))\n",
    "        \n",
    "        driver.implicitly_wait(4)\n",
    "\n",
    "        naver_main = driver.page_source\n",
    "\n",
    "        parsed_naver = bs(naver_main, 'lxml')\n",
    "\n",
    "        ### for each age range - all, 10s to 50s\n",
    "        \n",
    "        for age in table_list:\n",
    "            tmplist = [age, input_date+\" \"+time] #first sections of row that would look [age, date time, rank1, rank2, ...rank 20] \n",
    "            table_all = parsed_naver.find(\"div\", {\"data-age\":\"{}\".format(age)}) #get specific age of parsed info\n",
    "            for i in table_all.find_all(\"span\", {\"class\": \"title\"}):\n",
    "                tmplist.append(i.text) #append rank1 to rank20 to previously made list\n",
    "            daily.append(tmplist) #append entire row\n",
    "        driver.quit()\n",
    "    \n",
    "    df = pd.DataFrame(daily, columns = colnames) ### create dataframe of 1 month x all age range x all time in day\n",
    "    \n",
    "    gbq.to_gbq(df, table_name, project_id, if_exists='append', credentials = credentials) ### append df to DB\n",
    "    \n",
    "    print(\"completed \" + input_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get year/ month/ date of last update made to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_line = \"select Datetime from NaverCrawler.Trending20 order by Datetime desc limit 1\" \n",
    "\n",
    "query_job = client.query(sql_line)\n",
    "\n",
    "results = query_job.result()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "tmp = str(df[0])\n",
    "\n",
    "year = int(tmp[6:10])\n",
    "month = int(tmp[11:13])\n",
    "date_of_month = int(tmp[14:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get date_list from last updated date in DB to current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, date\n",
    "\n",
    "def daterange(start_date, end_date): ## gets range of dates between two days (input type = date)\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "now = datetime.now()\n",
    "        \n",
    "start_date = date(int(year), int(month), int(date_of_month)) # input from above cell\n",
    "start_date = start_date + timedelta(days = 1) #start date from one day after the last record in DB\n",
    "end_date = now.date() \n",
    "\n",
    "date_list = []\n",
    "for single_date in daterange(start_date, end_date): #for every date in between start_date and end_date, fill in date_list\n",
    "    date_list.append(single_date.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for every date to be updated, update DB\n",
    "## each day costs approx. 3 minutes (local computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "\n",
      "1it [00:15, 15.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 2019-09-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "\n",
      "1it [00:05,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 2019-09-21\n"
     ]
    }
   ],
   "source": [
    "for date in date_list:\n",
    "    daily_update(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## errors and limitations:\n",
    "### operating for 30 date causes next operation to crash\n",
    "### Occasional crash in calling chromedriver found\n",
    "### Bug found in 0it and 1it, but not sure what 0it means. There should be one iteration to bigquery (data, however, is gathered without error)\n",
    "### Bug found \"ReadTimeout: HTTPSConnectionPool(host='bigquery.googleapis.com', port=443): Read timed out. (read timeout=60)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
